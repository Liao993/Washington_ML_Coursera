{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Week 4: Ridge Regression (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will implement ridge regression via gradient descent. You will:\n",
    "* Convert an SFrame into a Numpy array\n",
    "* Write a Numpy function to compute the derivative of the regression weights with respect to a single feature\n",
    "* Write gradient descent function to compute the regression weights given an initial weight vector, step size, tolerance, and L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire up Turi Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of Turi Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in house sales data\n",
    "\n",
    "Dataset is from house sales in King County, the region where the city of Seattle, WA is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = turicreate.SFrame('~/Desktop/2.self_learning/coursera_washington/module2_regression/data/home_data.sframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do any \"feature engineering\" like creating new features or adjusting existing ones we should do this directly using the SFrames as seen in the first notebook of Week 2. For this notebook, however, we will work with the existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import useful functions from previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Week 2, we convert the SFrame into a 2D Numpy array. Copy and paste `get_numpy_data()` from the second notebook of Week 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # note this allows us to refer to numpy as np instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(data_sframe, features, output):\n",
    "    \n",
    "    data_sframe['constant'] = 1\n",
    "    feature_ = ['constant'] + features\n",
    "    \n",
    "    feature_chosen = data_sframe[feature_]\n",
    "    feature_matrix = feature_chosen.to_numpy()\n",
    "    \n",
    "    output_ = data_sframe[output]\n",
    "    output_array = output_.to_numpy()\n",
    "    \n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(feature, output) = get_numpy_data(sales,['sqft_living'],'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, copy and paste the `predict_output()` function to compute the predictions for an entire matrix of features given the matrix and the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    \n",
    "    prediction = np.dot(feature_matrix, weights)\n",
    "    \n",
    "    return(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1181., 2571.,  771., ..., 1021., 1601., 1021.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_output(feature,np.array([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to move to computing the derivative of the regression cost function. Recall that the cost function is the sum over the data points of the squared difference between an observed output and a predicted output, plus the L2 penalty term.\n",
    "```\n",
    "Cost(w)\n",
    "= SUM[ (prediction - output)^2 ]\n",
    "+ l2_penalty*(w[0]^2 + w[1]^2 + ... + w[k]^2).\n",
    "```\n",
    "\n",
    "Since the derivative of a sum is the sum of the derivatives, we can take the derivative of the first part (the RSS) as we did in the notebook for the unregularized case in Week 2 and add the derivative of the regularization part.  As we saw, the derivative of the RSS with respect to `w[i]` can be written as: \n",
    "```\n",
    "2*SUM[ error*[feature_i] ].\n",
    "```\n",
    "The derivative of the regularization term with respect to `w[i]` is:\n",
    "```\n",
    "2*l2_penalty*w[i].\n",
    "```\n",
    "Summing both, we get\n",
    "```\n",
    "2*SUM[ error*[feature_i] ] + 2*l2_penalty*w[i].\n",
    "```\n",
    "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself, plus `2*l2_penalty*w[i]`. \n",
    "\n",
    "**We will not regularize the constant.**  Thus, in the case of the constant, the derivative is just twice the sum of the errors (without the `2*l2_penalty*w[0]` term).\n",
    "\n",
    "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors, plus `2*l2_penalty*w[i]`.\n",
    "\n",
    "With this in mind complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points).  To decide when to we are dealing with the constant (so we don't regularize it) we added the extra parameter to the call `feature_is_constant` which you should set to `True` when computing the derivative of the constant and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative_ridge(errors, feature, weight, l2_penalty, feature_is_constant):\n",
    "    errornp = np.array(errors)\n",
    "    featurenp = np.array(feature)\n",
    "    l2_penaltynp = np.array(l2_penalty)\n",
    "    weightnp = np.array(weight)\n",
    "    \n",
    "    # If feature_is_constant is True, derivative is twice the dot product of errors and feature\n",
    "    if feature_is_constant is True:\n",
    "        ab = np.dot(errornp,featurenp)\n",
    "        derivative = 2 * ab\n",
    "    elif feature_is_constant is False:\n",
    "        ac = np.dot(errors,feature) \n",
    "        wl = np.dot(l2_penaltynp,weight)\n",
    "        derivative = 2 * (ac + wl)\n",
    "    else:\n",
    "        print('please make sure feature_is_constant is true or false')\n",
    "        \n",
    "        \n",
    "    # Otherwise, derivative is twice the dot product plus 2*l2_penalty*weight\n",
    "    \n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your feature derivartive run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature is not constant\n",
      "-56554166782350.0\n",
      "-56554166782350.0\n",
      "\n",
      "feature is constant\n",
      "-22446749336.0\n",
      "-22446749336.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') \n",
    "my_weights = np.array([1., 10.])\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "errors = test_predictions - example_output # prediction errors\n",
    "\n",
    "# next two lines should print the same values\n",
    "print('feature is not constant')\n",
    "print(feature_derivative_ridge(errors, example_features[:,1], my_weights[1], 1, False))\n",
    "print(np.sum(errors*example_features[:,1])*2+my_weights[1]*1*2)\n",
    "print('')\n",
    "\n",
    "# next two lines should print the same values\n",
    "print('feature is constant')\n",
    "print(feature_derivative_ridge(errors, example_features[:,0], my_weights[0], 1, True))\n",
    "print(np.sum(errors)*2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of *increase* and therefore the negative gradient is the direction of *decrease* and we're trying to *minimize* a cost function. \n",
    "\n",
    "The amount by which we move in the negative gradient *direction*  is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. Unlike in Week 2, this time we will set a **maximum number of iterations** and take gradient steps until we reach this maximum number. If no maximum number is supplied, the maximum should be set 100 by default. (Use default parameter values in Python.)\n",
    "\n",
    "With this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent, we update the weight for each feature before computing our stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, l2_penalty, max_iterations=100):\n",
    "    print('Starting gradient descent with l2_penalty = ' + str(l2_penalty))\n",
    "    \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    iteration = 0 # iteration counter\n",
    "    print_frequency = 1  # for adjusting frequency of debugging output\n",
    "\n",
    "    \n",
    "    #while not reached maximum number of iterations:\n",
    "    for it in range(max_iterations):\n",
    "\n",
    "        prediction = predict_output(feature_matrix, weights)\n",
    "    \n",
    "        error = prediction - output  # compute the errors as predictions - output\n",
    "        \n",
    "       \n",
    "        for i in range(len(weights)): \n",
    "          \n",
    "            if i is 0:\n",
    "                feature_is_constant = True\n",
    "            else:\n",
    "                feature_is_constant = False\n",
    "            \n",
    "            derivative = feature_derivative_ridge(error, feature_matrix[:,i], \n",
    "                                     weights[i], l2_penalty, feature_is_constant)\n",
    "        \n",
    "            weights[i] -= step_size * derivative\n",
    "            \n",
    "        #print('Done with gradient descent at iteration ', it+1)\n",
    "        #print('Learned weights = ', str(weights))\n",
    "\n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing effect of L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 penalty gets its name because it causes weights to have small L2 norms than otherwise. Let's see how large weights get penalized. Let us consider a simple model with 1 feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_features = ['sqft_living']\n",
    "my_output = 'price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the dataset into training set and test set. Make sure to use `seed=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = sales.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will only use `'sqft_living'` to predict `'price'`. Use the `get_numpy_data` function to get a Numpy versions of your data with only this feature, for both the `train_data` and the `test_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n",
    "(simple_test_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the parameters for our optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = np.array([0., 0.])\n",
    "step_size = 1e-12\n",
    "max_iterations=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider no regularization.  Set the `l2_penalty` to `0.0` and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\n",
    "\n",
    "`simple_weights_0_penalty`\n",
    "\n",
    "we'll use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 0\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [1.87526989e-02 4.73325137e+01]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.40823824e-02 8.61473080e+01]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [4.66050390e-02 1.17977189e+02]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [5.68258070e-02 1.44079125e+02]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [6.51589227e-02 1.65483889e+02]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [7.19440783e-02 1.83036761e+02]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [7.74598370e-02 1.97430906e+02]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [8.19346329e-02 2.09234754e+02]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [8.55557925e-02 2.18914442e+02]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [8.84769319e-02 2.26852222e+02]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [9.08240230e-02 2.33361559e+02]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [9.27003689e-02 2.38699509e+02]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [9.41906823e-02 2.43076868e+02]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [9.53644317e-02 2.46666500e+02]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [9.62785844e-02 2.49610160e+02]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [9.69798560e-02 2.52024094e+02]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [9.75065557e-02 2.54003629e+02]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [9.78900984e-02 2.55626937e+02]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [9.81562460e-02 2.56958122e+02]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [9.83261243e-02 2.58049754e+02]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [9.84170574e-02 2.58944942e+02]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [9.84432520e-02 2.59679036e+02]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [9.84163580e-02 2.60281026e+02]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [9.83459289e-02 2.60774685e+02]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [9.82397992e-02 2.61179508e+02]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [9.81043934e-02 2.61511481e+02]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [9.79449797e-02 2.61783714e+02]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [9.77658787e-02 2.62006957e+02]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [9.75706330e-02 2.62190027e+02]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [9.73621480e-02 2.62340152e+02]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [9.71428062e-02 2.62463261e+02]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [9.69145613e-02 2.62564217e+02]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [9.66790155e-02 2.62647005e+02]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [9.64374826e-02 2.62714894e+02]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [9.61910400e-02 2.62770567e+02]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [9.59405712e-02 2.62816221e+02]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [9.56868009e-02 2.62853660e+02]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [9.54303230e-02 2.62884361e+02]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [9.51716248e-02 2.62909538e+02]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [9.49111060e-02 2.62930184e+02]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [9.46490941e-02 2.62947114e+02]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [9.43858578e-02 2.62960998e+02]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [9.41216174e-02 2.62972383e+02]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [9.38565537e-02 2.62981720e+02]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [9.35908148e-02 2.62989376e+02]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [9.33245222e-02 2.62995655e+02]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [9.30577756e-02 2.63000804e+02]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [9.27906566e-02 2.63005026e+02]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [9.25232323e-02 2.63008488e+02]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [9.22555576e-02 2.63011328e+02]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [9.19876775e-02 2.63013656e+02]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [9.17196291e-02 2.63015566e+02]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [9.14514426e-02 2.63017132e+02]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [9.11831428e-02 2.63018416e+02]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [9.09147502e-02 2.63019469e+02]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [9.06462815e-02 2.63020332e+02]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [9.03777503e-02 2.63021040e+02]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [9.01091679e-02 2.63021621e+02]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [8.98405436e-02 2.63022097e+02]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [8.95718848e-02 2.63022488e+02]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [8.93031977e-02 2.63022808e+02]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [8.90344875e-02 2.63023071e+02]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [8.87657583e-02 2.63023286e+02]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [8.84970136e-02 2.63023463e+02]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [8.82282561e-02 2.63023608e+02]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [8.79594881e-02 2.63023727e+02]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [8.76907115e-02 2.63023824e+02]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [8.74219279e-02 2.63023904e+02]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [8.71531385e-02 2.63023970e+02]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [8.68843444e-02 2.63024024e+02]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [8.66155463e-02 2.63024068e+02]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [8.63467452e-02 2.63024104e+02]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [8.60779414e-02 2.63024134e+02]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [8.58091354e-02 2.63024158e+02]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [8.55403277e-02 2.63024178e+02]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [8.52715186e-02 2.63024195e+02]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [8.50027083e-02 2.63024208e+02]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [8.47338970e-02 2.63024219e+02]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [8.44650849e-02 2.63024228e+02]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [8.41962722e-02 2.63024236e+02]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [8.39274589e-02 2.63024242e+02]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [8.36586452e-02 2.63024247e+02]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [8.33898312e-02 2.63024251e+02]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [8.31210168e-02 2.63024255e+02]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [8.28522023e-02 2.63024258e+02]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [8.25833875e-02 2.63024260e+02]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [8.23145725e-02 2.63024262e+02]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [8.20457575e-02 2.63024264e+02]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [8.17769423e-02 2.63024265e+02]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [8.15081270e-02 2.63024266e+02]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [8.12393117e-02 2.63024267e+02]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [8.09704963e-02 2.63024268e+02]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [8.07016808e-02 2.63024268e+02]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [8.04328654e-02 2.63024269e+02]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [8.01640498e-02 2.63024269e+02]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [7.98952343e-02 2.63024270e+02]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [7.96264187e-02 2.63024270e+02]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [7.93576031e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [7.90887876e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [7.88199720e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [7.85511563e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [7.82823407e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [7.80135251e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [7.77447095e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [7.74758938e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [7.72070782e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [7.69382626e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [7.66694469e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [7.64006313e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [7.61318156e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [7.58630000e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [7.55941844e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [7.53253687e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [7.50565531e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [7.47877375e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [7.45189218e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [7.42501062e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [7.39812906e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [7.37124749e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [7.34436593e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [7.31748437e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [7.29060281e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [7.26372124e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [7.23683968e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [7.20995812e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [7.18307656e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [7.15619499e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [7.12931343e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [7.10243187e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [7.07555031e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [7.04866875e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [7.02178719e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [6.99490563e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [6.96802407e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [6.94114250e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [6.91426094e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [6.88737938e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [6.86049782e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [6.83361626e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [6.80673470e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [6.77985314e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [6.75297158e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [6.72609002e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [6.69920847e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [6.67232691e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [6.64544535e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [6.61856379e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [6.59168223e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [6.56480067e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [6.53791911e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [6.51103755e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [6.48415600e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [6.45727444e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [6.43039288e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [6.40351132e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [6.37662977e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [6.34974821e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [6.32286665e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [6.29598509e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [6.26910354e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [6.24222198e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [6.21534042e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [6.18845887e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [6.16157731e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [6.13469576e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [6.10781420e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [6.08093264e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [6.05405109e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [6.02716953e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [6.00028798e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [5.97340642e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [5.94652487e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [5.91964331e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [5.89276176e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [5.8658802e-02 2.6302428e+02]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [5.83899865e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [5.81211709e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [5.78523554e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [5.75835399e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [5.73147243e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [5.70459088e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [5.67770933e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [5.65082777e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [5.62394622e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [5.59706467e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [5.57018311e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [5.54330156e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [5.51642001e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [5.48953845e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [5.46265690e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [5.43577535e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [5.40889380e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [5.38201225e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [5.35513070e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [5.32824914e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [5.30136759e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [5.27448604e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [5.24760449e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [5.22072294e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [5.19384139e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [5.16695984e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [5.14007829e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [5.11319674e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [5.08631519e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [5.05943364e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [5.03255209e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [5.00567054e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [4.97878899e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [4.95190744e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [4.92502589e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [4.89814434e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [4.87126279e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [4.84438124e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [4.81749969e-02 2.63024284e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [4.79061815e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [4.76373660e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [4.73685505e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [4.70997350e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [4.68309195e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [4.65621041e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [4.62932886e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [4.60244731e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [4.57556576e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [4.54868422e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [4.52180267e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [4.49492112e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [4.46803958e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [4.44115803e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [4.41427648e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [4.38739494e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [4.36051339e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [4.33363185e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [4.30675030e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [4.27986875e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [4.25298721e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [4.22610566e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [4.19922412e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [4.17234257e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [4.14546103e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [4.11857949e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [4.09169794e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [4.06481640e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [4.03793485e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [4.01105331e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [3.98417176e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [3.95729022e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [3.93040868e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [3.90352713e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [3.87664559e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [3.84976405e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [3.82288250e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [3.79600096e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [3.76911942e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [3.74223788e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [3.71535633e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [3.68847479e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [3.66159325e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [3.63471171e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [3.60783017e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [3.58094863e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [3.55406708e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [3.52718554e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [3.50030400e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [3.47342246e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [3.44654092e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [3.41965938e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [3.39277784e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [3.3658963e-02 2.6302429e+02]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [3.33901476e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [3.31213322e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [3.28525168e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [3.25837014e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [3.2314886e-02 2.6302429e+02]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [3.20460706e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [3.17772552e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [3.15084398e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [3.12396244e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [3.09708090e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [3.07019937e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [3.04331783e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [3.01643629e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [2.98955475e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [2.96267321e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [2.93579167e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [2.90891014e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [2.88202860e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [2.85514706e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [2.82826552e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [2.80138399e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [2.77450245e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [2.74762091e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [2.72073938e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [2.69385784e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [2.66697630e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [2.64009477e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [2.61321323e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [2.58633170e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [2.55945016e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [2.53256863e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [2.50568709e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [2.47880555e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [2.45192402e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [2.42504248e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [2.39816095e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [2.37127942e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [2.34439788e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [2.31751635e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [2.29063481e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [2.26375328e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [2.23687174e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [2.20999021e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [2.18310868e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [2.15622714e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [2.12934561e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [2.10246408e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [2.07558254e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [2.04870101e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [2.02181948e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [1.99493795e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [1.96805641e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [1.94117488e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [1.91429335e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [1.88741182e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [1.86053029e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [1.83364876e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [1.80676722e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [1.77988569e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [1.75300416e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [1.72612263e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [1.69924110e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [1.67235957e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [1.64547804e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [1.61859651e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [1.59171498e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [1.56483345e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [1.53795192e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [1.51107039e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [1.48418886e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [1.45730733e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [1.43042580e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [1.40354427e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [1.37666274e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [1.34978121e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [1.32289968e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [1.29601816e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [1.26913663e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [1.24225510e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [1.21537357e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [1.18849204e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [1.16161052e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [1.13472899e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [1.10784746e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [1.08096593e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [1.05408441e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [1.02720288e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [1.00032135e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [9.73439826e-03 2.63024299e+02]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [9.465583e-03 2.630243e+02]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [9.19676773e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [8.92795247e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [8.65913721e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [8.39032195e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [8.12150669e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [7.85269144e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [7.58387618e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [7.31506093e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [7.04624568e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [6.77743043e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [6.50861518e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [6.23979994e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [5.97098469e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [5.70216945e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [5.43335421e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [5.16453897e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [4.89572373e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [4.62690849e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [4.35809325e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [4.08927802e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [3.82046279e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [3.55164755e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [3.28283233e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [3.01401710e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [2.74520187e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [2.47638665e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [2.20757142e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [1.93875620e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [1.66994098e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [1.40112576e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [1.13231054e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [8.63495327e-04 2.63024303e+02]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [5.94680113e-04 2.63024303e+02]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [3.25864900e-04 2.63024303e+02]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [5.70496884e-05 2.63024303e+02]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [-2.11765521e-04  2.63024303e+02]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [-4.80580730e-04  2.63024304e+02]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [-7.49395936e-04  2.63024304e+02]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [-1.01821114e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [-1.28702635e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [-1.55584155e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [-1.82465675e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [-2.09347195e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [-2.36228714e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [-2.63110234e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [-2.89991753e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [-3.16873273e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [-3.43754792e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [-3.70636311e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [-3.97517830e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [-4.24399348e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [-4.51280867e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [-4.78162385e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [-5.05043903e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [-5.31925422e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [-5.58806939e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [-5.85688457e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [-6.12569975e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [-6.39451492e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [-6.66333010e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [-6.93214527e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [-7.20096044e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [-7.46977561e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [-7.73859077e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [-8.00740594e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [-8.27622110e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [-8.54503626e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [-8.81385142e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [-9.08266658e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [-9.35148174e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [-9.62029690e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [-9.88911205e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [-1.01579272e-02  2.63024307e+02]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [-1.04267424e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [-1.06955575e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [-1.09643727e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [-1.12331878e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [-1.15020029e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [-1.17708181e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [-1.20396332e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [-1.23084484e-02  2.63024308e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [-1.25772635e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [-1.28460787e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [-1.31148938e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [-1.33837089e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [-1.36525241e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [-1.39213392e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [-1.41901543e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [-1.44589694e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [-1.47277846e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [-1.49965997e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [-1.52654148e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [-1.55342299e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [-1.58030451e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [-1.60718602e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [-1.63406753e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [-1.66094904e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [-1.68783055e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [-1.71471206e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [-1.74159358e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [-1.76847509e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [-1.79535660e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [-1.82223811e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [-1.84911962e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [-1.87600113e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [-1.90288264e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [-1.92976415e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [-1.95664566e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [-1.98352717e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [-2.01040868e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [-2.03729019e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [-2.06417170e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [-2.09105321e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [-2.11793472e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [-2.14481622e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [-2.17169773e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [-2.19857924e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [-2.22546075e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [-2.25234226e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [-2.27922377e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [-2.30610527e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [-2.33298678e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [-2.35986829e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [-2.38674980e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [-2.41363130e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [-2.44051281e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [-2.46739432e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [-2.49427583e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [-2.52115733e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [-2.54803884e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [-2.57492035e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [-2.60180185e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [-2.62868336e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [-2.65556486e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [-2.68244637e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [-2.70932788e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [-2.73620938e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [-2.76309089e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [-2.78997239e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [-2.81685390e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [-2.84373540e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [-2.87061691e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [-2.89749841e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [-2.92437991e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [-2.95126142e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [-2.97814292e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [-3.00502443e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [-3.03190593e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [-3.05878743e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [-3.08566894e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [-3.11255044e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [-3.13943194e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [-3.16631345e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [-3.19319495e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [-3.22007645e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [-3.24695796e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [-3.27383946e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [-3.30072096e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [-3.32760246e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [-3.35448396e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [-3.38136547e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [-3.40824697e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [-3.43512847e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [-3.46200997e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [-3.48889147e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [-3.51577297e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [-3.54265447e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [-3.56953597e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [-3.59641748e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [-3.62329898e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [-3.65018048e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [-3.67706198e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [-3.70394348e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [-3.73082498e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [-3.75770648e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [-3.78458798e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [-3.81146947e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [-3.83835097e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [-3.86523247e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [-3.89211397e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [-3.91899547e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [-3.94587697e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [-3.97275847e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [-3.99963997e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [-4.02652146e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [-4.05340296e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [-4.08028446e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [-4.10716596e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [-4.13404745e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [-4.16092895e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [-4.18781045e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [-4.21469195e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [-4.24157344e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [-4.26845494e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [-4.29533644e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [-4.32221793e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [-4.34909943e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [-4.37598093e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [-4.40286242e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [-4.42974392e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [-4.45662541e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [-4.48350691e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [-4.51038840e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [-4.53726990e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [-4.56415139e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [-4.59103289e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [-4.61791438e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [-4.64479588e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [-4.67167737e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [-4.69855887e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [-4.72544036e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [-4.75232186e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [-4.77920335e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [-4.80608484e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [-4.83296634e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [-4.85984783e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [-4.88672932e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [-4.91361082e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [-4.94049231e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [-4.96737380e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [-4.99425529e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [-5.02113679e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [-5.04801828e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [-5.07489977e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [-5.10178126e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [-5.12866275e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [-5.15554425e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [-5.18242574e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [-5.20930723e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [-5.23618872e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [-5.26307021e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [-5.28995170e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [-5.31683319e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [-5.34371468e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [-5.37059617e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [-5.39747766e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [-5.42435915e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [-5.45124064e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [-5.47812213e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [-5.50500362e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [-5.53188511e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [-5.55876660e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [-5.58564809e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [-5.61252958e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [-5.63941107e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [-5.66629256e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [-5.69317405e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [-5.72005554e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [-5.74693702e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [-5.77381851e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [-5.80070000e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [-5.82758149e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [-5.85446298e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [-5.88134446e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [-5.90822595e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [-5.93510744e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [-5.96198893e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [-5.98887041e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [-6.01575190e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [-6.04263339e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [-6.06951487e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [-6.09639636e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [-6.12327784e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [-6.15015933e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [-6.17704082e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [-6.20392230e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [-6.23080379e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [-6.25768527e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [-6.28456676e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [-6.31144824e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [-6.33832973e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [-6.36521121e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [-6.39209270e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [-6.41897418e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [-6.44585567e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [-6.47273715e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [-6.49961864e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [-6.52650012e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [-6.5533816e-02  2.6302433e+02]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [-6.58026309e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [-6.60714457e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [-6.63402605e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [-6.66090754e-02  2.63024330e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [-6.68778902e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [-6.7146705e-02  2.6302433e+02]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [-6.74155198e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [-6.76843347e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [-6.79531495e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [-6.82219643e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [-6.84907791e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [-6.87595940e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [-6.90284088e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [-6.92972236e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [-6.95660384e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [-6.98348532e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [-7.01036680e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [-7.03724828e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [-7.06412976e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [-7.09101124e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [-7.11789273e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [-7.14477421e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [-7.17165569e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [-7.19853717e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [-7.22541865e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [-7.25230013e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [-7.27918160e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [-7.30606308e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [-7.33294456e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [-7.35982604e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [-7.38670752e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [-7.41358900e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [-7.44047048e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [-7.46735196e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [-7.49423344e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [-7.52111491e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [-7.54799639e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [-7.57487787e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [-7.60175935e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [-7.62864083e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [-7.65552230e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [-7.68240378e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [-7.70928526e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [-7.73616673e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [-7.76304821e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [-7.78992969e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [-7.81681116e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [-7.84369264e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [-7.87057412e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [-7.89745559e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [-7.92433707e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [-7.95121854e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [-7.97810002e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [-8.00498150e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [-8.03186297e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [-8.05874445e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [-8.08562592e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [-8.11250740e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [-8.13938887e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [-8.16627034e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [-8.19315182e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [-8.22003329e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [-8.24691477e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [-8.27379624e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [-8.30067771e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [-8.32755919e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [-8.35444066e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [-8.38132213e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [-8.40820361e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [-8.43508508e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [-8.46196655e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [-8.48884803e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [-8.51572950e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [-8.54261097e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [-8.56949244e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [-8.59637392e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [-8.62325539e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [-8.65013686e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [-8.67701833e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [-8.70389980e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [-8.73078127e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [-8.75766274e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [-8.78454422e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [-8.81142569e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [-8.83830716e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [-8.86518863e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [-8.89207010e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [-8.91895157e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [-8.94583304e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [-8.97271451e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [-8.99959598e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [-9.02647745e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [-9.05335892e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [-9.08024039e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [-9.10712186e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [-9.13400332e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [-9.16088479e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [-9.18776626e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [-9.21464773e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [-9.2415292e-02  2.6302434e+02]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [-9.26841067e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [-9.29529214e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [-9.32217360e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [-9.34905507e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [-9.37593654e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [-9.40281801e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [-9.42969947e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [-9.45658094e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [-9.48346241e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [-9.51034387e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [-9.53722534e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [-9.56410681e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [-9.59098827e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [-9.61786974e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [-9.64475121e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [-9.67163267e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [-9.69851414e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [-9.72539560e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [-9.75227707e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [-9.77915853e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [-9.80604000e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [-9.83292146e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [-9.85980293e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [-9.88668439e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [-9.91356586e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [-9.94044732e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [-9.96732879e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [-9.99421025e-02  2.63024344e+02]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [-1.00210917e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [-1.00479732e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [-1.00748546e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [-1.01017361e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [-1.01286176e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [-1.01554990e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [-1.01823805e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [-1.02092620e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [-1.02361434e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [-1.02630249e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [-1.02899063e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [-1.03167878e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [-1.03436693e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [-1.03705507e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [-1.03974322e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [-1.04243137e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [-1.04511951e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [-1.04780766e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [-1.05049580e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [-1.05318395e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [-1.05587210e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [-1.05856024e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [-1.06124839e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [-1.06393653e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [-1.06662468e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [-1.06931283e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [-1.07200097e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [-1.07468912e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [-1.07737726e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [-1.08006541e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [-1.08275356e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [-1.08544170e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [-1.08812985e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [-1.09081799e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [-1.09350614e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [-1.09619429e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [-1.09888243e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [-1.10157058e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [-1.10425872e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [-1.10694687e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [-1.10963501e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [-1.11232316e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [-1.11501131e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [-1.11769945e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [-1.12038760e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [-1.12307574e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [-1.12576389e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [-1.12845203e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [-1.13114018e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [-1.13382833e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [-1.13651647e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [-1.13920462e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [-1.14189276e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [-1.14458091e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [-1.14726905e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [-1.1499572e-01  2.6302435e+02]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [-1.15264534e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [-1.15533349e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [-1.15802164e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [-1.16070978e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [-1.16339793e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [-1.16608607e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [-1.16877422e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [-1.17146236e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [-1.17415051e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [-1.17683865e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [-1.17952680e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [-1.18221494e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [-1.18490309e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [-1.18759124e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [-1.19027938e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [-1.19296753e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [-1.19565567e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [-1.19834382e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [-1.20103196e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [-1.20372011e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [-1.20640825e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [-1.20909640e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [-1.21178454e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [-1.21447269e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [-1.21716083e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [-1.21984898e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [-1.22253712e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [-1.22522527e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [-1.22791341e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [-1.23060156e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [-1.23328970e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [-1.23597785e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [-1.23866599e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [-1.24135414e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [-1.24404228e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [-1.24673043e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [-1.24941857e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [-1.25210672e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [-1.25479486e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [-1.25748301e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [-1.26017115e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [-1.26285930e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [-1.26554744e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [-1.26823559e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [-1.27092373e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [-1.27361188e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [-1.27630002e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [-1.27898817e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [-1.28167631e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [-1.28436446e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [-1.28705260e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [-1.28974075e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [-1.29242889e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [-1.29511704e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [-1.29780518e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [-1.30049333e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [-1.30318147e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [-1.30586962e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [-1.30855776e-01  2.63024356e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [-1.31124590e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [-1.31393405e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [-1.31662219e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [-1.31931034e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [-1.32199848e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [-1.32468663e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [-1.32737477e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [-1.33006292e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [-1.33275106e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [-1.33543921e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [-1.33812735e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [-1.34081549e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [-1.34350364e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [-1.34619178e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [-1.34887993e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [-1.35156807e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [-1.35425622e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [-1.35694436e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [-1.35963250e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [-1.36232065e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [-1.36500879e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [-1.36769694e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [-1.37038508e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [-1.37307323e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [-1.37576137e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [-1.37844952e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [-1.38113766e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [-1.38382580e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [-1.38651395e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [-1.38920209e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [-1.39189024e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [-1.39457838e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [-1.39726652e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [-1.39995467e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [-1.40264281e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [-1.40533096e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [-1.4080191e-01  2.6302436e+02]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [-1.41070724e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [-1.41339539e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [-1.41608353e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [-1.41877168e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [-1.42145982e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [-1.42414796e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [-1.42683611e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [-1.42952425e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [-1.43221240e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [-1.43490054e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [-1.43758868e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [-1.44027683e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [-1.44296497e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [-1.44565312e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [-1.44834126e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [-1.45102940e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [-1.45371755e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [-1.45640569e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [-1.45909384e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [-1.46178198e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [-1.46447012e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [-1.46715827e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [-1.46984641e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [-1.47253455e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [-1.47522270e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [-1.47791084e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [-1.48059898e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [-1.48328713e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [-1.48597527e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [-1.48866342e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [-1.49135156e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [-1.49403970e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [-1.49672785e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [-1.49941599e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [-1.50210413e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [-1.50479228e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [-1.50748042e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [-1.51016856e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [-1.51285671e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [-1.51554485e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [-1.51823299e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [-1.52092114e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [-1.52360928e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [-1.52629742e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [-1.52898557e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [-1.53167371e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [-1.53436186e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [-1.53705000e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [-1.53973814e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [-1.54242628e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [-1.54511443e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [-1.54780257e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [-1.55049071e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [-1.55317886e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [-1.55586700e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [-1.55855514e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [-1.56124329e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [-1.56393143e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [-1.56661957e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [-1.56930772e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [-1.57199586e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [-1.57468400e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [-1.57737215e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [-1.58006029e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [-1.58274843e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [-1.58543658e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [-1.58812472e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [-1.59081286e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [-1.59350100e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [-1.59618915e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [-1.59887729e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [-1.60156543e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [-1.60425358e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [-1.60694172e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [-1.60962986e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [-1.61231801e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [-1.61500615e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [-1.61769429e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [-1.62038243e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [-1.62307058e-01  2.63024369e+02]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [-1.62575872e-01  2.63024369e+02]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [-1.62844686e-01  2.63024369e+02]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [-1.63113501e-01  2.63024369e+02]\n"
     ]
    }
   ],
   "source": [
    "simple_weights_0_penalty = ridge_regression_gradient_descent(simple_feature_matrix,\n",
    "                                                             output = output,\n",
    "                                                             initial_weights = initial_weights, \n",
    "                                                             step_size = step_size,\n",
    "                                                             l2_penalty = 0,\n",
    "                                                             max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.63113501e-01,  2.63024369e+02])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weights_0_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's consider high regularization.  Set the `l2_penalty` to `1e11` and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\n",
    "\n",
    "`simple_weights_high_penalty`\n",
    "\n",
    "we'll use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 100000000000.0\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [1.87526989e-02 4.73325137e+01]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.40823824e-02 7.66808053e+01]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [4.72896420e-02 9.48780684e+01]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [5.91809029e-02 1.06161191e+02]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [7.02561853e-02 1.13157235e+02]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [8.08255241e-02 1.17495097e+02]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [9.10811550e-02 1.20184767e+02]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [1.01142273e-01 1.21852482e+02]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [1.11082784e-01 1.22886540e+02]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [1.20948513e-01 1.23527702e+02]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [  0.13076787 123.92525071]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [  0.14055848 124.17174806]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [  0.15033127 124.32458682]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [  0.160093   124.41935304]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [  0.16984787 124.47811166]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [  0.1795985  124.51454395]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [  0.18934649 124.53713291]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [  0.19909285 124.55113838]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [  0.2088382 124.5598217]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [  0.21858291 124.56520504]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [  0.22832724 124.56854225]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [  0.23807132 124.57061077]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [  0.24781526 124.57189264]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [  0.2575591  124.57268675]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [  0.26730289 124.57317843]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [  0.27704663 124.57348259]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [  0.28679036 124.57367047]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [  0.29653407 124.57378627]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [  0.30627778 124.57385736]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [  0.31602147 124.57390074]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [  0.32576517 124.57392693]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [  0.33550886 124.57394246]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [  0.34525255 124.57395139]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [  0.35499624 124.57395622]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [  0.36473993 124.57395851]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [  0.37448362 124.57395923]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [  0.38422731 124.57395897]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [  0.393971  124.5739581]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [  0.40371468 124.57395686]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [  0.41345837 124.57395538]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [  0.42320206 124.57395377]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [  0.43294575 124.57395206]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [  0.44268943 124.57395029]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [  0.45243312 124.57394849]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [  0.4621768  124.57394667]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [  0.47192049 124.57394484]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [  0.48166418 124.573943  ]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [  0.49140786 124.57394115]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [  0.50115155 124.5739393 ]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [  0.51089523 124.57393745]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [  0.52063892 124.5739356 ]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [  0.5303826  124.57393375]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [  0.54012629 124.57393189]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [  0.54986997 124.57393004]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [  0.55961366 124.57392819]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [  0.56935734 124.57392633]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [  0.57910102 124.57392448]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [  0.58884471 124.57392262]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [  0.59858839 124.57392077]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [  0.60833208 124.57391891]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [  0.61807576 124.57391706]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [  0.62781944 124.5739152 ]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [  0.63756312 124.57391335]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [  0.64730681 124.57391149]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [  0.65705049 124.57390964]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [  0.66679417 124.57390779]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [  0.67653785 124.57390593]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [  0.68628153 124.57390408]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [  0.69602522 124.57390222]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [  0.7057689  124.57390037]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [  0.71551258 124.57389851]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [  0.72525626 124.57389666]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [  0.73499994 124.5738948 ]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [  0.74474362 124.57389295]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [  0.7544873  124.57389109]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [  0.76423098 124.57388924]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [  0.77397466 124.57388739]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [  0.78371834 124.57388553]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [  0.79346202 124.57388368]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [  0.8032057  124.57388182]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [  0.81294938 124.57387997]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [  0.82269306 124.57387811]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [  0.83243673 124.57387626]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [  0.84218041 124.5738744 ]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [  0.85192409 124.57387255]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [  0.86166777 124.57387069]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [  0.87141145 124.57386884]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [  0.88115512 124.57386699]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [  0.8908988  124.57386513]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [  0.90064248 124.57386328]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [  0.91038615 124.57386142]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [  0.92012983 124.57385957]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [  0.92987351 124.57385771]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [  0.93961718 124.57385586]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [  0.94936086 124.573854  ]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [  0.95910454 124.57385215]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [  0.96884821 124.57385029]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [  0.97859189 124.57384844]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [  0.98833556 124.57384659]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [  0.99807924 124.57384473]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [  1.00782291 124.57384288]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [  1.01756659 124.57384102]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [  1.02731026 124.57383917]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [  1.03705394 124.57383731]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [  1.04679761 124.57383546]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [  1.05654128 124.5738336 ]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [  1.06628496 124.57383175]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [  1.07602863 124.57382989]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [  1.0857723  124.57382804]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [  1.09551598 124.57382619]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [  1.10525965 124.57382433]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [  1.11500332 124.57382248]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [  1.124747   124.57382062]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [  1.13449067 124.57381877]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [  1.14423434 124.57381691]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [  1.15397801 124.57381506]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [  1.16372168 124.5738132 ]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [  1.17346535 124.57381135]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [  1.18320903 124.57380949]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [  1.1929527  124.57380764]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [  1.20269637 124.57380579]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [  1.21244004 124.57380393]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [  1.22218371 124.57380208]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [  1.23192738 124.57380022]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [  1.24167105 124.57379837]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [  1.25141472 124.57379651]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [  1.26115839 124.57379466]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [  1.27090206 124.5737928 ]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [  1.28064573 124.57379095]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [  1.2903894  124.57378909]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [  1.30013306 124.57378724]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [  1.30987673 124.57378539]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [  1.3196204  124.57378353]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [  1.32936407 124.57378168]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [  1.33910774 124.57377982]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [  1.3488514  124.57377797]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [  1.35859507 124.57377611]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [  1.36833874 124.57377426]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [  1.37808241 124.5737724 ]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [  1.38782607 124.57377055]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [  1.39756974 124.57376869]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [  1.40731341 124.57376684]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [  1.41705707 124.57376499]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [  1.42680074 124.57376313]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [  1.4365444  124.57376128]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [  1.44628807 124.57375942]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [  1.45603174 124.57375757]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [  1.4657754  124.57375571]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [  1.47551907 124.57375386]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [  1.48526273 124.573752  ]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [  1.4950064  124.57375015]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [  1.50475006 124.57374829]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [  1.51449372 124.57374644]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [  1.52423739 124.57374458]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [  1.53398105 124.57374273]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [  1.54372472 124.57374088]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [  1.55346838 124.57373902]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [  1.56321204 124.57373717]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [  1.57295571 124.57373531]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [  1.58269937 124.57373346]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [  1.59244303 124.5737316 ]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [  1.60218669 124.57372975]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [  1.61193036 124.57372789]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [  1.62167402 124.57372604]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [  1.63141768 124.57372418]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [  1.64116134 124.57372233]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [  1.650905   124.57372048]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [  1.66064866 124.57371862]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [  1.67039232 124.57371677]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [  1.68013598 124.57371491]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [  1.68987965 124.57371306]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [  1.69962331 124.5737112 ]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [  1.70936697 124.57370935]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [  1.71911063 124.57370749]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [  1.72885429 124.57370564]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [  1.73859795 124.57370378]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [  1.7483416  124.57370193]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [  1.75808526 124.57370008]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [  1.76782892 124.57369822]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [  1.77757258 124.57369637]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [  1.78731624 124.57369451]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [  1.7970599  124.57369266]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [  1.80680356 124.5736908 ]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [  1.81654721 124.57368895]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [  1.82629087 124.57368709]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [  1.83603453 124.57368524]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [  1.84577819 124.57368338]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [  1.85552184 124.57368153]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [  1.8652655  124.57367968]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [  1.87500916 124.57367782]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [  1.88475281 124.57367597]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [  1.89449647 124.57367411]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [  1.90424013 124.57367226]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [  1.91398378 124.5736704 ]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [  1.92372744 124.57366855]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [  1.93347109 124.57366669]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [  1.94321475 124.57366484]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [  1.9529584  124.57366298]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [  1.96270206 124.57366113]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [  1.97244571 124.57365928]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [  1.98218937 124.57365742]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [  1.99193302 124.57365557]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [  2.00167667 124.57365371]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [  2.01142033 124.57365186]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [  2.02116398 124.57365   ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [  2.03090764 124.57364815]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [  2.04065129 124.57364629]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [  2.05039494 124.57364444]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [  2.06013859 124.57364258]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [  2.06988225 124.57364073]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [  2.0796259  124.57363888]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [  2.08936955 124.57363702]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [  2.0991132  124.57363517]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [  2.10885686 124.57363331]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [  2.11860051 124.57363146]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [  2.12834416 124.5736296 ]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [  2.13808781 124.57362775]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [  2.14783146 124.57362589]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [  2.15757511 124.57362404]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [  2.16731876 124.57362218]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [  2.17706241 124.57362033]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [  2.18680606 124.57361848]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [  2.19654971 124.57361662]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [  2.20629336 124.57361477]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [  2.21603701 124.57361291]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [  2.22578066 124.57361106]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [  2.23552431 124.5736092 ]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [  2.24526796 124.57360735]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [  2.25501161 124.57360549]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [  2.26475526 124.57360364]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [  2.2744989  124.57360178]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [  2.28424255 124.57359993]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [  2.2939862  124.57359808]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [  2.30372985 124.57359622]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [  2.31347349 124.57359437]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [  2.32321714 124.57359251]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [  2.33296079 124.57359066]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [  2.34270444 124.5735888 ]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [  2.35244808 124.57358695]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [  2.36219173 124.57358509]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [  2.37193538 124.57358324]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [  2.38167902 124.57358138]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [  2.39142267 124.57357953]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [  2.40116631 124.57357768]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [  2.41090996 124.57357582]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [  2.4206536  124.57357397]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [  2.43039725 124.57357211]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [  2.44014089 124.57357026]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [  2.44988454 124.5735684 ]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [  2.45962818 124.57356655]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [  2.46937183 124.57356469]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [  2.47911547 124.57356284]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [  2.48885911 124.57356098]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [  2.49860276 124.57355913]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [  2.5083464  124.57355728]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [  2.51809004 124.57355542]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [  2.52783369 124.57355357]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [  2.53757733 124.57355171]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [  2.54732097 124.57354986]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [  2.55706461 124.573548  ]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [  2.56680826 124.57354615]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [  2.5765519  124.57354429]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [  2.58629554 124.57354244]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [  2.59603918 124.57354058]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [  2.60578282 124.57353873]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [  2.61552646 124.57353688]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [  2.62527011 124.57353502]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [  2.63501375 124.57353317]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [  2.64475739 124.57353131]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [  2.65450103 124.57352946]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [  2.66424467 124.5735276 ]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [  2.67398831 124.57352575]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [  2.68373195 124.57352389]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [  2.69347559 124.57352204]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [  2.70321923 124.57352018]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [  2.71296286 124.57351833]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [  2.7227065  124.57351648]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [  2.73245014 124.57351462]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [  2.74219378 124.57351277]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [  2.75193742 124.57351091]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [  2.76168106 124.57350906]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [  2.77142469 124.5735072 ]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [  2.78116833 124.57350535]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [  2.79091197 124.57350349]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [  2.80065561 124.57350164]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [  2.81039924 124.57349978]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [  2.82014288 124.57349793]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [  2.82988652 124.57349608]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [  2.83963015 124.57349422]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [  2.84937379 124.57349237]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [  2.85911743 124.57349051]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [  2.86886106 124.57348866]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [  2.8786047 124.5734868]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [  2.88834833 124.57348495]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [  2.89809197 124.57348309]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [  2.9078356  124.57348124]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [  2.91757924 124.57347938]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [  2.92732287 124.57347753]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [  2.93706651 124.57347568]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [  2.94681014 124.57347382]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [  2.95655377 124.57347197]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [  2.96629741 124.57347011]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [  2.97604104 124.57346826]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [  2.98578467 124.5734664 ]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [  2.99552831 124.57346455]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [  3.00527194 124.57346269]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [  3.01501557 124.57346084]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [  3.02475921 124.57345898]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [  3.03450284 124.57345713]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [  3.04424647 124.57345528]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [  3.0539901  124.57345342]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [  3.06373373 124.57345157]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [  3.07347737 124.57344971]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [  3.083221   124.57344786]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [  3.09296463 124.573446  ]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [  3.10270826 124.57344415]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [  3.11245189 124.57344229]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [  3.12219552 124.57344044]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [  3.13193915 124.57343858]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [  3.14168278 124.57343673]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [  3.15142641 124.57343488]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [  3.16117004 124.57343302]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [  3.17091367 124.57343117]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [  3.1806573  124.57342931]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [  3.19040093 124.57342746]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [  3.20014456 124.5734256 ]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [  3.20988818 124.57342375]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [  3.21963181 124.57342189]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [  3.22937544 124.57342004]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [  3.23911907 124.57341818]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [  3.2488627  124.57341633]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [  3.25860632 124.57341448]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [  3.26834995 124.57341262]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [  3.27809358 124.57341077]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [  3.28783721 124.57340891]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [  3.29758083 124.57340706]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [  3.30732446 124.5734052 ]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [  3.31706809 124.57340335]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [  3.32681171 124.57340149]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [  3.33655534 124.57339964]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [  3.34629896 124.57339778]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [  3.35604259 124.57339593]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [  3.36578622 124.57339407]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [  3.37552984 124.57339222]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [  3.38527347 124.57339037]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [  3.39501709 124.57338851]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [  3.40476071 124.57338666]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [  3.41450434 124.5733848 ]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [  3.42424796 124.57338295]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [  3.43399159 124.57338109]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [  3.44373521 124.57337924]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [  3.45347883 124.57337738]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [  3.46322246 124.57337553]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [  3.47296608 124.57337367]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [  3.4827097  124.57337182]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [  3.49245333 124.57336997]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [  3.50219695 124.57336811]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [  3.51194057 124.57336626]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [  3.52168419 124.5733644 ]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [  3.53142781 124.57336255]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [  3.54117144 124.57336069]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [  3.55091506 124.57335884]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [  3.56065868 124.57335698]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [  3.5704023  124.57335513]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [  3.58014592 124.57335327]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [  3.58988954 124.57335142]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [  3.59963316 124.57334957]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [  3.60937678 124.57334771]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [  3.6191204  124.57334586]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [  3.62886402 124.573344  ]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [  3.63860764 124.57334215]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [  3.64835126 124.57334029]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [  3.65809488 124.57333844]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [  3.6678385  124.57333658]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [  3.67758212 124.57333473]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [  3.68732574 124.57333287]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [  3.69706936 124.57333102]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [  3.70681297 124.57332917]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [  3.71655659 124.57332731]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [  3.72630021 124.57332546]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [  3.73604383 124.5733236 ]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [  3.74578744 124.57332175]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [  3.75553106 124.57331989]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [  3.76527468 124.57331804]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [  3.77501829 124.57331618]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [  3.78476191 124.57331433]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [  3.79450553 124.57331247]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [  3.80424914 124.57331062]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [  3.81399276 124.57330877]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [  3.82373638 124.57330691]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [  3.83347999 124.57330506]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [  3.84322361 124.5733032 ]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [  3.85296722 124.57330135]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [  3.86271084 124.57329949]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [  3.87245445 124.57329764]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [  3.88219807 124.57329578]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [  3.89194168 124.57329393]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [  3.90168529 124.57329207]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [  3.91142891 124.57329022]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [  3.92117252 124.57328837]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [  3.93091614 124.57328651]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [  3.94065975 124.57328466]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [  3.95040336 124.5732828 ]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [  3.96014697 124.57328095]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [  3.96989059 124.57327909]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [  3.9796342  124.57327724]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [  3.98937781 124.57327538]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [  3.99912142 124.57327353]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [  4.00886504 124.57327167]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [  4.01860865 124.57326982]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [  4.02835226 124.57326797]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [  4.03809587 124.57326611]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [  4.04783948 124.57326426]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [  4.05758309 124.5732624 ]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [  4.0673267  124.57326055]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [  4.07707031 124.57325869]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [  4.08681392 124.57325684]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [  4.09655753 124.57325498]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [  4.10630114 124.57325313]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [  4.11604475 124.57325128]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [  4.12578836 124.57324942]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [  4.13553197 124.57324757]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [  4.14527558 124.57324571]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [  4.15501919 124.57324386]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [  4.1647628 124.573242 ]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [  4.17450641 124.57324015]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [  4.18425001 124.57323829]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [  4.19399362 124.57323644]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [  4.20373723 124.57323458]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [  4.21348084 124.57323273]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [  4.22322444 124.57323088]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [  4.23296805 124.57322902]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [  4.24271166 124.57322717]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [  4.25245527 124.57322531]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [  4.26219887 124.57322346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [  4.27194248 124.5732216 ]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [  4.28168608 124.57321975]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [  4.29142969 124.57321789]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [  4.3011733  124.57321604]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [  4.3109169  124.57321418]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [  4.32066051 124.57321233]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [  4.33040411 124.57321048]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [  4.34014772 124.57320862]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [  4.34989132 124.57320677]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [  4.35963493 124.57320491]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [  4.36937853 124.57320306]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [  4.37912213 124.5732012 ]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [  4.38886574 124.57319935]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [  4.39860934 124.57319749]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [  4.40835294 124.57319564]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [  4.41809655 124.57319378]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [  4.42784015 124.57319193]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [  4.43758375 124.57319008]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [  4.44732736 124.57318822]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [  4.45707096 124.57318637]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [  4.46681456 124.57318451]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [  4.47655816 124.57318266]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [  4.48630177 124.5731808 ]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [  4.49604537 124.57317895]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [  4.50578897 124.57317709]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [  4.51553257 124.57317524]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [  4.52527617 124.57317338]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [  4.53501977 124.57317153]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [  4.54476337 124.57316968]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [  4.55450697 124.57316782]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [  4.56425057 124.57316597]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [  4.57399417 124.57316411]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [  4.58373777 124.57316226]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [  4.59348137 124.5731604 ]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [  4.60322497 124.57315855]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [  4.61296857 124.57315669]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [  4.62271217 124.57315484]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [  4.63245577 124.57315298]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [  4.64219937 124.57315113]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [  4.65194296 124.57314928]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [  4.66168656 124.57314742]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [  4.67143016 124.57314557]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [  4.68117376 124.57314371]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [  4.69091736 124.57314186]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [  4.70066095 124.57314   ]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [  4.71040455 124.57313815]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [  4.72014815 124.57313629]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [  4.72989174 124.57313444]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [  4.73963534 124.57313258]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [  4.74937894 124.57313073]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [  4.75912253 124.57312888]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [  4.76886613 124.57312702]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [  4.77860972 124.57312517]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [  4.78835332 124.57312331]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [  4.79809692 124.57312146]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [  4.80784051 124.5731196 ]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [  4.81758411 124.57311775]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [  4.8273277  124.57311589]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [  4.83707129 124.57311404]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [  4.84681489 124.57311218]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [  4.85655848 124.57311033]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [  4.86630208 124.57310848]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [  4.87604567 124.57310662]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [  4.88578926 124.57310477]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [  4.89553286 124.57310291]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [  4.90527645 124.57310106]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [  4.91502004 124.5730992 ]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [  4.92476364 124.57309735]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [  4.93450723 124.57309549]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [  4.94425082 124.57309364]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [  4.95399441 124.57309178]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [  4.963738   124.57308993]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [  4.9734816  124.57308808]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [  4.98322519 124.57308622]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [  4.99296878 124.57308437]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [  5.00271237 124.57308251]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [  5.01245596 124.57308066]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [  5.02219955 124.5730788 ]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [  5.03194314 124.57307695]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [  5.04168673 124.57307509]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [  5.05143032 124.57307324]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [  5.06117391 124.57307138]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [  5.0709175  124.57306953]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [  5.08066109 124.57306768]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [  5.09040468 124.57306582]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [  5.10014827 124.57306397]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [  5.10989186 124.57306211]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [  5.11963544 124.57306026]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [  5.12937903 124.5730584 ]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [  5.13912262 124.57305655]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [  5.14886621 124.57305469]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [  5.1586098  124.57305284]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [  5.16835338 124.57305098]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [  5.17809697 124.57304913]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [  5.18784056 124.57304728]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [  5.19758414 124.57304542]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [  5.20732773 124.57304357]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [  5.21707132 124.57304171]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [  5.2268149  124.57303986]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [  5.23655849 124.573038  ]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [  5.24630208 124.57303615]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [  5.25604566 124.57303429]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [  5.26578925 124.57303244]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [  5.27553283 124.57303058]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [  5.28527642 124.57302873]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [  5.29502    124.57302688]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [  5.30476359 124.57302502]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [  5.31450717 124.57302317]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [  5.32425076 124.57302131]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [  5.33399434 124.57301946]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [  5.34373792 124.5730176 ]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [  5.35348151 124.57301575]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [  5.36322509 124.57301389]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [  5.37296867 124.57301204]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [  5.38271226 124.57301018]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [  5.39245584 124.57300833]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [  5.40219942 124.57300648]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [  5.411943   124.57300462]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [  5.42168659 124.57300277]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [  5.43143017 124.57300091]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [  5.44117375 124.57299906]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [  5.45091733 124.5729972 ]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [  5.46066091 124.57299535]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [  5.47040449 124.57299349]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [  5.48014808 124.57299164]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [  5.48989166 124.57298978]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [  5.49963524 124.57298793]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [  5.50937882 124.57298608]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [  5.5191224  124.57298422]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [  5.52886598 124.57298237]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [  5.53860956 124.57298051]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [  5.54835314 124.57297866]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [  5.55809672 124.5729768 ]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [  5.56784029 124.57297495]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [  5.57758387 124.57297309]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [  5.58732745 124.57297124]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [  5.59707103 124.57296938]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [  5.60681461 124.57296753]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [  5.61655819 124.57296568]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [  5.62630176 124.57296382]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [  5.63604534 124.57296197]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [  5.64578892 124.57296011]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [  5.6555325  124.57295826]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [  5.66527607 124.5729564 ]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [  5.67501965 124.57295455]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [  5.68476323 124.57295269]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [  5.6945068  124.57295084]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [  5.70425038 124.57294898]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [  5.71399396 124.57294713]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [  5.72373753 124.57294528]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [  5.73348111 124.57294342]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [  5.74322468 124.57294157]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [  5.75296826 124.57293971]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [  5.76271183 124.57293786]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [  5.77245541 124.572936  ]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [  5.78219898 124.57293415]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [  5.79194256 124.57293229]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [  5.80168613 124.57293044]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [  5.81142971 124.57292859]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [  5.82117328 124.57292673]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [  5.83091685 124.57292488]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [  5.84066043 124.57292302]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [  5.850404   124.57292117]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [  5.86014757 124.57291931]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [  5.86989115 124.57291746]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [  5.87963472 124.5729156 ]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [  5.88937829 124.57291375]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [  5.89912186 124.57291189]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [  5.90886543 124.57291004]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [  5.91860901 124.57290819]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [  5.92835258 124.57290633]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [  5.93809615 124.57290448]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [  5.94783972 124.57290262]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [  5.95758329 124.57290077]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [  5.96732686 124.57289891]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [  5.97707043 124.57289706]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [  5.986814  124.5728952]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [  5.99655757 124.57289335]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [  6.00630114 124.57289149]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [  6.01604471 124.57288964]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [  6.02578828 124.57288779]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [  6.03553185 124.57288593]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [  6.04527542 124.57288408]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [  6.05501899 124.57288222]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [  6.06476256 124.57288037]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [  6.07450613 124.57287851]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [  6.08424969 124.57287666]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [  6.09399326 124.5728748 ]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [  6.10373683 124.57287295]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [  6.1134804  124.57287109]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [  6.12322396 124.57286924]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [  6.13296753 124.57286739]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [  6.1427111  124.57286553]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [  6.15245467 124.57286368]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [  6.16219823 124.57286182]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [  6.1719418  124.57285997]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [  6.18168537 124.57285811]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [  6.19142893 124.57285626]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [  6.2011725 124.5728544]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [  6.21091606 124.57285255]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [  6.22065963 124.57285069]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [  6.23040319 124.57284884]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [  6.24014676 124.57284699]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [  6.24989032 124.57284513]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [  6.25963389 124.57284328]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [  6.26937745 124.57284142]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [  6.27912102 124.57283957]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [  6.28886458 124.57283771]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [  6.29860814 124.57283586]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [  6.30835171 124.572834  ]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [  6.31809527 124.57283215]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [  6.32783883 124.57283029]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [  6.3375824  124.57282844]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [  6.34732596 124.57282659]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [  6.35706952 124.57282473]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [  6.36681308 124.57282288]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [  6.37655665 124.57282102]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [  6.38630021 124.57281917]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [  6.39604377 124.57281731]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [  6.40578733 124.57281546]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [  6.41553089 124.5728136 ]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [  6.42527445 124.57281175]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [  6.43501801 124.57280989]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [  6.44476158 124.57280804]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [  6.45450514 124.57280619]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [  6.4642487  124.57280433]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [  6.47399226 124.57280248]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [  6.48373582 124.57280062]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [  6.49347938 124.57279877]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [  6.50322294 124.57279691]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [  6.51296649 124.57279506]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [  6.52271005 124.5727932 ]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [  6.53245361 124.57279135]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [  6.54219717 124.57278949]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [  6.55194073 124.57278764]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [  6.56168429 124.57278579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [  6.57142785 124.57278393]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [  6.5811714  124.57278208]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [  6.59091496 124.57278022]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [  6.60065852 124.57277837]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [  6.61040208 124.57277651]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [  6.62014563 124.57277466]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [  6.62988919 124.5727728 ]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [  6.63963275 124.57277095]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [  6.6493763 124.5727691]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [  6.65911986 124.57276724]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [  6.66886341 124.57276539]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [  6.67860697 124.57276353]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [  6.68835053 124.57276168]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [  6.69809408 124.57275982]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [  6.70783764 124.57275797]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [  6.71758119 124.57275611]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [  6.72732475 124.57275426]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [  6.7370683 124.5727524]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [  6.74681185 124.57275055]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [  6.75655541 124.5727487 ]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [  6.76629896 124.57274684]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [  6.77604252 124.57274499]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [  6.78578607 124.57274313]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [  6.79552962 124.57274128]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [  6.80527318 124.57273942]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [  6.81501673 124.57273757]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [  6.82476028 124.57273571]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [  6.83450383 124.57273386]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [  6.84424739 124.572732  ]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [  6.85399094 124.57273015]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [  6.86373449 124.5727283 ]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [  6.87347804 124.57272644]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [  6.88322159 124.57272459]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [  6.89296515 124.57272273]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [  6.9027087  124.57272088]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [  6.91245225 124.57271902]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [  6.9221958  124.57271717]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [  6.93193935 124.57271531]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [  6.9416829  124.57271346]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [  6.95142645 124.5727116 ]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [  6.96117    124.57270975]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [  6.97091355 124.5727079 ]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [  6.9806571  124.57270604]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [  6.99040065 124.57270419]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [  7.0001442  124.57270233]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [  7.00988774 124.57270048]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [  7.01963129 124.57269862]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [  7.02937484 124.57269677]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [  7.03911839 124.57269491]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [  7.04886194 124.57269306]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [  7.05860549 124.5726912 ]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [  7.06834903 124.57268935]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [  7.07809258 124.5726875 ]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [  7.08783613 124.57268564]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [  7.09757967 124.57268379]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [  7.10732322 124.57268193]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [  7.11706677 124.57268008]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [  7.12681031 124.57267822]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [  7.13655386 124.57267637]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [  7.14629741 124.57267451]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [  7.15604095 124.57267266]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [  7.1657845 124.5726708]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [  7.17552804 124.57266895]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [  7.18527159 124.5726671 ]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [  7.19501513 124.57266524]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [  7.20475868 124.57266339]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [  7.21450222 124.57266153]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [  7.22424577 124.57265968]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [  7.23398931 124.57265782]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [  7.24373285 124.57265597]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [  7.2534764  124.57265411]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [  7.26321994 124.57265226]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [  7.27296349 124.5726504 ]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [  7.28270703 124.57264855]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [  7.29245057 124.5726467 ]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [  7.30219411 124.57264484]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [  7.31193766 124.57264299]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [  7.3216812  124.57264113]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [  7.33142474 124.57263928]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [  7.34116828 124.57263742]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [  7.35091182 124.57263557]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [  7.36065537 124.57263371]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [  7.37039891 124.57263186]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [  7.38014245 124.57263001]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [  7.38988599 124.57262815]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [  7.39962953 124.5726263 ]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [  7.40937307 124.57262444]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [  7.41911661 124.57262259]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [  7.42886015 124.57262073]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [  7.43860369 124.57261888]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [  7.44834723 124.57261702]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [  7.45809077 124.57261517]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [  7.46783431 124.57261331]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [  7.47757785 124.57261146]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [  7.48732139 124.57260961]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [  7.49706492 124.57260775]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [  7.50680846 124.5726059 ]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [  7.516552   124.57260404]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [  7.52629554 124.57260219]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [  7.53603908 124.57260033]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [  7.54578261 124.57259848]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [  7.55552615 124.57259662]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [  7.56526969 124.57259477]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [  7.57501323 124.57259291]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [  7.58475676 124.57259106]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [  7.5945003  124.57258921]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [  7.60424384 124.57258735]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [  7.61398737 124.5725855 ]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [  7.62373091 124.57258364]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [  7.63347444 124.57258179]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [  7.64321798 124.57257993]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [  7.65296151 124.57257808]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [  7.66270505 124.57257622]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [  7.67244858 124.57257437]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [  7.68219212 124.57257251]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [  7.69193565 124.57257066]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [  7.70167919 124.57256881]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [  7.71142272 124.57256695]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [  7.72116626 124.5725651 ]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [  7.73090979 124.57256324]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [  7.74065332 124.57256139]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [  7.75039686 124.57255953]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [  7.76014039 124.57255768]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [  7.76988392 124.57255582]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [  7.77962745 124.57255397]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [  7.78937099 124.57255211]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [  7.79911452 124.57255026]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [  7.80885805 124.57254841]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [  7.81860158 124.57254655]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [  7.82834511 124.5725447 ]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [  7.83808865 124.57254284]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [  7.84783218 124.57254099]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [  7.85757571 124.57253913]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [  7.86731924 124.57253728]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [  7.87706277 124.57253542]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [  7.8868063  124.57253357]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [  7.89654983 124.57253172]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [  7.90629336 124.57252986]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [  7.91603689 124.57252801]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [  7.92578042 124.57252615]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [  7.93552395 124.5725243 ]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [  7.94526748 124.57252244]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [  7.95501101 124.57252059]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [  7.96475454 124.57251873]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [  7.97449806 124.57251688]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [  7.98424159 124.57251502]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [  7.99398512 124.57251317]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [  8.00372865 124.57251132]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [  8.01347218 124.57250946]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [  8.0232157  124.57250761]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [  8.03295923 124.57250575]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [  8.04270276 124.5725039 ]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [  8.05244629 124.57250204]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [  8.06218981 124.57250019]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [  8.07193334 124.57249833]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [  8.08167686 124.57249648]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [  8.09142039 124.57249462]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [  8.10116392 124.57249277]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [  8.11090744 124.57249092]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [  8.12065097 124.57248906]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [  8.13039449 124.57248721]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [  8.14013802 124.57248535]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [  8.14988154 124.5724835 ]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [  8.15962507 124.57248164]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [  8.16936859 124.57247979]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [  8.17911212 124.57247793]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [  8.18885564 124.57247608]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [  8.19859916 124.57247422]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [  8.20834269 124.57247237]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [  8.21808621 124.57247052]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [  8.22782973 124.57246866]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [  8.23757326 124.57246681]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [  8.24731678 124.57246495]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [  8.2570603 124.5724631]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [  8.26680383 124.57246124]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [  8.27654735 124.57245939]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [  8.28629087 124.57245753]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [  8.29603439 124.57245568]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [  8.30577791 124.57245382]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [  8.31552143 124.57245197]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [  8.32526496 124.57245012]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [  8.33500848 124.57244826]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [  8.344752   124.57244641]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [  8.35449552 124.57244455]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [  8.36423904 124.5724427 ]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [  8.37398256 124.57244084]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [  8.38372608 124.57243899]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [  8.3934696  124.57243713]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [  8.40321312 124.57243528]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [  8.41295664 124.57243343]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [  8.42270016 124.57243157]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [  8.43244368 124.57242972]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [  8.44218719 124.57242786]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [  8.45193071 124.57242601]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [  8.46167423 124.57242415]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [  8.47141775 124.5724223 ]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [  8.48116127 124.57242044]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [  8.49090478 124.57241859]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [  8.5006483  124.57241673]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [  8.51039182 124.57241488]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [  8.52013534 124.57241303]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [  8.52987885 124.57241117]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [  8.53962237 124.57240932]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [  8.54936589 124.57240746]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [  8.5591094  124.57240561]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [  8.56885292 124.57240375]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [  8.57859643 124.5724019 ]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [  8.58833995 124.57240004]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [  8.59808347 124.57239819]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [  8.60782698 124.57239633]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [  8.6175705  124.57239448]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [  8.62731401 124.57239263]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [  8.63705753 124.57239077]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [  8.64680104 124.57238892]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [  8.65654455 124.57238706]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [  8.66628807 124.57238521]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [  8.67603158 124.57238335]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [  8.6857751 124.5723815]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [  8.69551861 124.57237964]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [  8.70526212 124.57237779]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [  8.71500564 124.57237593]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [  8.72474915 124.57237408]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [  8.73449266 124.57237223]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [  8.74423617 124.57237037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [  8.75397969 124.57236852]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [  8.7637232  124.57236666]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [  8.77346671 124.57236481]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [  8.78321022 124.57236295]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [  8.79295373 124.5723611 ]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [  8.80269724 124.57235924]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [  8.81244075 124.57235739]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [  8.82218426 124.57235554]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [  8.83192778 124.57235368]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [  8.84167129 124.57235183]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [  8.8514148  124.57234997]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [  8.86115831 124.57234812]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [  8.87090182 124.57234626]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [  8.88064532 124.57234441]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [  8.89038883 124.57234255]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [  8.90013234 124.5723407 ]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [  8.90987585 124.57233884]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [  8.91961936 124.57233699]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [  8.92936287 124.57233514]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [  8.93910638 124.57233328]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [  8.94884989 124.57233143]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [  8.95859339 124.57232957]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [  8.9683369  124.57232772]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [  8.97808041 124.57232586]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [  8.98782392 124.57232401]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [  8.99756742 124.57232215]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [  9.00731093 124.5723203 ]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [  9.01705444 124.57231844]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [  9.02679794 124.57231659]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [  9.03654145 124.57231474]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [  9.04628496 124.57231288]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [  9.05602846 124.57231103]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [  9.06577197 124.57230917]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [  9.07551547 124.57230732]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [  9.08525898 124.57230546]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [  9.09500248 124.57230361]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [  9.10474599 124.57230175]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [  9.11448949 124.5722999 ]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [  9.124233   124.57229804]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [  9.1339765  124.57229619]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [  9.14372    124.57229434]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [  9.15346351 124.57229248]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [  9.16320701 124.57229063]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [  9.17295052 124.57228877]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [  9.18269402 124.57228692]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [  9.19243752 124.57228506]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [  9.20218102 124.57228321]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [  9.21192453 124.57228135]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [  9.22166803 124.5722795 ]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [  9.23141153 124.57227765]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [  9.24115503 124.57227579]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [  9.25089853 124.57227394]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [  9.26064204 124.57227208]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [  9.27038554 124.57227023]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [  9.28012904 124.57226837]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [  9.28987254 124.57226652]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [  9.29961604 124.57226466]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [  9.30935954 124.57226281]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [  9.31910304 124.57226095]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [  9.32884654 124.5722591 ]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [  9.33859004 124.57225725]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [  9.34833354 124.57225539]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [  9.35807704 124.57225354]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [  9.36782054 124.57225168]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [  9.37756404 124.57224983]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [  9.38730754 124.57224797]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [  9.39705104 124.57224612]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [  9.40679453 124.57224426]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [  9.41653803 124.57224241]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [  9.42628153 124.57224055]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [  9.43602503 124.5722387 ]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [  9.44576853 124.57223685]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [  9.45551202 124.57223499]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [  9.46525552 124.57223314]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [  9.47499902 124.57223128]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [  9.48474252 124.57222943]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [  9.49448601 124.57222757]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [  9.50422951 124.57222572]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [  9.513973   124.57222386]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [  9.5237165  124.57222201]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [  9.53346    124.57222015]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [  9.54320349 124.5722183 ]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [  9.55294699 124.57221645]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [  9.56269048 124.57221459]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [  9.57243398 124.57221274]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [  9.58217747 124.57221088]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [  9.59192097 124.57220903]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [  9.60166446 124.57220717]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [  9.61140796 124.57220532]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [  9.62115145 124.57220346]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [  9.63089494 124.57220161]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [  9.64063844 124.57219976]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [  9.65038193 124.5721979 ]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [  9.66012542 124.57219605]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [  9.66986892 124.57219419]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [  9.67961241 124.57219234]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [  9.6893559  124.57219048]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [  9.69909939 124.57218863]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [  9.70884289 124.57218677]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [  9.71858638 124.57218492]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [  9.72832987 124.57218306]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [  9.73807336 124.57218121]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [  9.74781685 124.57217936]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [  9.75756034 124.5721775 ]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [  9.76730383 124.57217565]\n"
     ]
    }
   ],
   "source": [
    "simple_weights_high_penalty = ridge_regression_gradient_descent(simple_feature_matrix,\n",
    "                                                             output = output,\n",
    "                                                             initial_weights = initial_weights, \n",
    "                                                             step_size = step_size,\n",
    "                                                             l2_penalty = 1e11,\n",
    "                                                             max_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.76730383, 124.57217565])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weights_high_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will plot the two learned models.  (The blue line is for the model with no regularization and the red line is for the one with high regularization.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc1eaa8e950>,\n",
       " <matplotlib.lines.Line2D at 0x7fc1eaa82450>,\n",
       " <matplotlib.lines.Line2D at 0x7fc1eaa8ea50>,\n",
       " <matplotlib.lines.Line2D at 0x7fc1eaa8ec90>,\n",
       " <matplotlib.lines.Line2D at 0x7fc1eaa8ed50>,\n",
       " <matplotlib.lines.Line2D at 0x7fc1eaa8ee10>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(simple_feature_matrix,output,'k.',\n",
    "         simple_feature_matrix,predict_output(simple_feature_matrix, simple_weights_0_penalty),'b-',\n",
    "        simple_feature_matrix,predict_output(simple_feature_matrix, simple_weights_high_penalty),'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the RSS on the TEST data for the following three sets of weights:\n",
    "1. The initial weights (all zeros)\n",
    "2. The weights learned with no regularization\n",
    "3. The weights learned with high regularization\n",
    "\n",
    "Which weights perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "(simple_test_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 0\n"
     ]
    }
   ],
   "source": [
    "simple_weights_0_penalty = ridge_regression_gradient_descent(simple_test_feature_matrix,\n",
    "                                                             output = test_output,\n",
    "                                                             initial_weights = initial_weights, \n",
    "                                                             step_size = step_size,\n",
    "                                                             l2_penalty = 0,\n",
    "                                                             max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.78815788e-02, 2.63357579e+02])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weights_0_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 100000000000.0\n"
     ]
    }
   ],
   "source": [
    "simple_weights_high_penalty = ridge_regression_gradient_descent(simple_test_feature_matrix,\n",
    "                                                             output = test_output,\n",
    "                                                             initial_weights = initial_weights, \n",
    "                                                             step_size = step_size,\n",
    "                                                             l2_penalty = 1e11,\n",
    "                                                             max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.7690843 , 47.04801437])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weights_high_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question1-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTIONS***\n",
    "1. What is the value of the coefficient for `sqft_living` that you learned with no regularization, rounded to 1 decimal place?  What about the one with high regularization?\n",
    "2. Comparing the lines you fit with the with no regularization versus high regularization, which one is steeper?\n",
    "3. What are the RSS on the test data for each of the set of weights above (initial, no regularization, high regularization)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.78815788e-02, 2.63357579e+02])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weights_0_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.4"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(2.63357579e+02,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.7690843 , 47.04801437])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weights_high_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(47.04801437,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question_3 : 0_penalty is steeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rss for initial_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RSS with initial weights (all zeros) for simple regression 1784273282524564.0\n"
     ]
    }
   ],
   "source": [
    "initial_weights = np.array([0.0,0.0])\n",
    "simple_prediction_initial = predict_output(simple_test_feature_matrix,initial_weights) \n",
    "error = test_output - simple_prediction_initial\n",
    "rss_initial = sum((error)**2)\n",
    "print(\" RSS with initial weights (all zeros) for simple regression\",rss_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RSS with no regularization for simple regression 275721227214294.12\n"
     ]
    }
   ],
   "source": [
    "weight_0 = simple_weights_0_penalty\n",
    "test_prediction = predict_output(simple_test_feature_matrix,weight_0)\n",
    "\n",
    "error = test_output - test_prediction\n",
    "rss_0penalty = sum(error**2)\n",
    "print(\" RSS with no regularization for simple regression\",rss_0penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RSS with regularization for simple regression 1293407855062754.8\n"
     ]
    }
   ],
   "source": [
    "weight_high = simple_weights_high_penalty\n",
    "test_prediction = predict_output(simple_test_feature_matrix,weight_high)\n",
    "\n",
    "error = test_output - test_prediction\n",
    "rss_highpenalty = sum(error**2)\n",
    "print(\" RSS with regularization for simple regression\", rss_highpenalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer_4 : 275721227214294.12 / 2.75e14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a multiple regression with L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider a model with 2 features: `['sqft_living', 'sqft_living15']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create Numpy versions of your training and test data with these two features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = ['sqft_living', 'sqft_living15'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n",
    "(test_feature_matrix, test_output) = get_numpy_data(test_data, model_features, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00e+00, 1.18e+03, 1.34e+03],\n",
       "       [1.00e+00, 2.57e+03, 1.69e+03]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to re-inialize the weights, since we have one extra parameter. Let us also set the step size and maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = np.array([0.0,0.0,0.0])\n",
    "step_size = 1e-12\n",
    "max_iterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider no regularization.  Set the `l2_penalty` to `0.0` and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\n",
    "\n",
    "`multiple_weights_0_penalty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 0\n"
     ]
    }
   ],
   "source": [
    "multiple_weights_0_penalty = ridge_regression_gradient_descent(feature_matrix,\n",
    "                                                             output = output,\n",
    "                                                             initial_weights = initial_weights, \n",
    "                                                             step_size = step_size,\n",
    "                                                             l2_penalty = 0,\n",
    "                                                             max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's consider high regularization.  Set the `l2_penalty` to `1e11` and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\n",
    "\n",
    "`multiple_weights_high_penalty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 100000000000.0\n"
     ]
    }
   ],
   "source": [
    "multiple_weights_high_penalty = ridge_regression_gradient_descent(feature_matrix,\n",
    "                                                             output = output,\n",
    "                                                             initial_weights = initial_weights, \n",
    "                                                             step_size = step_size,\n",
    "                                                             l2_penalty = 1e11,\n",
    "                                                             max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the house price for the 1st house in the test set using the no regularization and high regularization models. (Remember that python starts indexing from 0.) How far is the prediction from the actual price?  Which weights perform best for the 1st house?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question5-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***QUIZ QUESTIONS***\n",
    "1. What is the value of the coefficient for `sqft_living` that you learned with no regularization, rounded to 1 decimal place?  What about the one with high regularization?\n",
    "2. What are the RSS on the test data for each of the set of weights above (initial, no regularization, high regularization)? \n",
    "3. We make prediction for the first house in the test set using two sets of weights (no regularization vs high regularization). Which weights make better prediction <u>for that particular house</u>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = ['sqft_living', 'sqft_living15'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n",
    "(test_feature_matrix, test_output) = get_numpy_data(test_data, model_features, my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.35743482, 243.0541689 ,  22.41481594])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_weights_0_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243.1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(243.0541689,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.7429658 , 91.48927361, 78.43658768])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_weights_high_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.5"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(91.48927361,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500404800579555.8\n"
     ]
    }
   ],
   "source": [
    "weight_high = multiple_weights_high_penalty\n",
    "test_prediction = predict_output(test_feature_matrix,weight_high)\n",
    "\n",
    "error = test_output - test_prediction\n",
    "rss = sum(error**2)\n",
    "print(rss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer_7 ; 5e14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the RSS on the TEST data for the following three sets of weights:\n",
    "1. The initial weights (all zeros)\n",
    "2. The weights learned with no regularization\n",
    "3. The weights learned with high regularization\n",
    "\n",
    "Which weights perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the RSS with initial weights (all zeros) 1784273282524564.0\n"
     ]
    }
   ],
   "source": [
    "prediction_initial = predict_output(test_feature_matrix,initial_weights) \n",
    "error = test_output - prediction_initial\n",
    "rss_initial = sum((error)**2)\n",
    "print(\" the RSS with initial weights (all zeros)\",rss_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the RSS with no regularization 274067618287244.97\n"
     ]
    }
   ],
   "source": [
    "prediction_nor = predict_output(test_feature_matrix,multiple_weights_0_penalty) \n",
    "error = test_output - prediction_nor\n",
    "rss_nor = sum((error)**2)\n",
    "print(\" the RSS with no regularization\",rss_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the RSS with high regularization 500404800579555.8\n"
     ]
    }
   ],
   "source": [
    "prediction_larger = predict_output(test_feature_matrix,multiple_weights_high_penalty) \n",
    "error = test_output - prediction_larger\n",
    "rss_larger = sum((error)**2)\n",
    "print(\" the RSS with high regularization\",rss_larger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the house price for the 1st house in the test set using the no regularization and high regularization models. (Remember that python starts indexing from 0.) How far is the prediction from the actual price? Which weights perform best for the 1st house?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387465.47646474396\n",
      "221900.0\n"
     ]
    }
   ],
   "source": [
    "print(prediction_nor[0])\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270453.5303048586\n",
      "221900.0\n"
     ]
    }
   ],
   "source": [
    "print(prediction_larger[0])\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the weights from the model **with regularization** perform best for the 1st house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. simple regression (sqft_living)**\n",
    "\n",
    "weight after 1000 iterition\n",
    "\n",
    "    no_regularization : [6.78815788e-02, 2.63357579e+02] \n",
    "\n",
    "    with_regularization : [ 3.7690843 , 47.04801437]\n",
    "\n",
    "RSS for test data\n",
    "\n",
    "    initial_weight : 1784273282524564.0 / 1.78e15\n",
    "\n",
    "    no_regularization : 275721227214294.12 / 2.75e15\n",
    "\n",
    "    with_regularization : 1293407855062754.8 / 1.29e15\n",
    "\n",
    "**2.multiple regression (sqft_living, sqft_living15)**\n",
    "\n",
    "weight after 1000 iterition\n",
    "\n",
    "    no_regularization : [ -0.35743482, 243.0541689 ,  22.41481594]\n",
    "\n",
    "    with_regularization : [ 6.7429658 , 91.48927361, 78.43658768]\n",
    "\n",
    "RSS for test data\n",
    "\n",
    "    initial_weight : 1784273282524564.0 / 1.78e15\n",
    "\n",
    "    no_regularization : 274067618287244.97 / 2.74e14\n",
    "\n",
    "    with_regularization : 500404800579555.8 / 5.00e14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.price for individual data**\n",
    "\n",
    "price for 1st house\n",
    "\n",
    "    true price : 221900\n",
    "\n",
    "    price no regularization : 387465.5\n",
    "\n",
    "    price with regularization : 27.453.5  (better)\n",
    "\n",
    "price for 1234th house\n",
    "\n",
    "    true price : 265900\n",
    "\n",
    "    price no regularization : 245085.8 9 (better)\n",
    "\n",
    "    price with regularization : 174510.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.39999389648438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
